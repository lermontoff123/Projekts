{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f776321-84bb-4016-a9a4-ea0a58234d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import optuna\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.utils.data as data_torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from ydata_profiling import ProfileReport # подробный разбор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a647c34-9f04-4dc2-b7c4-1598bdbdbc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_patch = '.\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e374b48-0d0b-4211-a90e-489f6e77325d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratify_data(filename):\n",
    "    \n",
    "    dataframe = pd.read_csv(filename).copy()\n",
    "    dataframe = dataframe.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    mask = dataframe['Age'].isna()\n",
    "    dataframe.loc[mask, 'Age'] = np.random.randint(10, 50, mask.sum())\n",
    "    dataframe['Embarked'] = dataframe['Embarked'].fillna(dataframe['Embarked'].mode()[0])\n",
    "    dataframe['Sex'] = dataframe['Sex'].map({'male':1,'female':0})\n",
    "    dataframe['Embarked'] = dataframe['Embarked'].map({'S':0,'C':1, 'Q':2})\n",
    "    \n",
    "    survived = dataframe['Survived']\n",
    "    \n",
    "    features = dataframe.drop(['Survived'], axis=1)\n",
    "    features = pd.get_dummies(features, columns=['Pclass', 'SibSp', 'Parch', 'Embarked'], dtype=int)\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    features = poly.fit_transform(features)\n",
    "        \n",
    "    features = RobustScaler().fit_transform(features)\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    \n",
    "    data_scaled = pd.DataFrame(data=features)\n",
    "        \n",
    "    X_train, X_val, y_train, y_val = train_test_split(data_scaled, survived, test_size=0.2, random_state=2, stratify=survived)\n",
    "    \n",
    "    X_train['Survived'] = y_train\n",
    "    X_val['Survived'] = y_val\n",
    "    \n",
    "    X_train.to_csv('train_data.csv', sep=',', index=False)\n",
    "    X_val.to_csv('val_data.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3d53fd-ff2c-4f79-8b6a-a89f73bd6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify_data(data_patch + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d792d296-1050-4133-8c46-fce206bb4510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TitanicDataset(data_torch.Dataset):\n",
    "    \n",
    "    def __init__(self, filename, Train=True):\n",
    "        self.dataframe = pd.read_csv(filename).copy()\n",
    "                      \n",
    "        self.Train = Train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if(self.Train):\n",
    "            survived = self.dataframe['Survived']\n",
    "            survived = np.array(survived)[idx]\n",
    "            \n",
    "        features = self.dataframe.drop(['Survived'], axis=1)\n",
    "        features = np.array(features)[idx]\n",
    "                \n",
    "        if(self.Train):\n",
    "            return features, survived\n",
    "        else:\n",
    "            return features\n",
    "          \n",
    "    def infoo(self):\n",
    "        return self.dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d665d3-9c21-478c-8ae1-18b335c0310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TitanicDataset(data_patch + 'train_data.csv')\n",
    "val_dataset = TitanicDataset(data_patch + 'val_data.csv')\n",
    "testing_dataset = TitanicDataset(data_patch + 'test.csv', Train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8706a68a-192e-40ea-890d-4f2c373029c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00000000e+00, 3.34003518e-01, 2.70496001e-02, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.13881826e-01, 2.02872001e-02,\n",
       "        0.00000000e+00, 3.85714286e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.85714286e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.37500000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.80281690e-01, 0.00000000e+00,\n",
       "        7.31680868e-04, 0.00000000e+00, 1.88548299e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.26931559e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.70496001e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.70496001e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3d3db0-e7f3-4365-b45e-6c9775f704da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 712 entries, 0 to 711\n",
      "Columns: 300 entries, 0 to Survived\n",
      "dtypes: float64(299), int64(1)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.infoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19de4e7e-7a97-4dec-abd9-18093efd4eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size_columns = len(next(iter(train_dataset))[0])\n",
    "hidden_layer1_coeff = 15\n",
    "hidden_layer2_coeff = 7\n",
    "hidden_layer3_coeff = 3.5\n",
    "hidden_layer4_coeff = 2\n",
    "#lr = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "hidden_layer1 = round(hidden_layer1_coeff * size_columns)\n",
    "hidden_layer2 = round(hidden_layer2_coeff * size_columns)\n",
    "hidden_layer3 = round(hidden_layer3_coeff * size_columns)\n",
    "hidden_layer4 = round(hidden_layer4_coeff * size_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92c7b18-c573-4baa-8ee7-b6ed053263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, col, a, b, c, d, out1, out2):\n",
    "        super(Net, self).__init__()\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Linear(in_features=col,out_features=a),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(a),\n",
    "            nn.Linear(in_features=a,out_features=b),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(b),\n",
    "            nn.Linear(in_features=b,out_features=c),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.BatchNorm1d(c),\n",
    "            nn.Linear(in_features=c,out_features=d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.BatchNorm1d(d),\n",
    "            nn.Linear(in_features=d,out_features=int(out1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(int(out1)),\n",
    "            nn.Linear(in_features=int(out1),out_features=out2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(out2),\n",
    "            nn.Linear(in_features=out2,out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.b1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35969110-ee8b-4be0-a99e-4e5b81a34eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(net.parameters(), lr = lr, momentum = momentum)\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68456f0f-3c84-4904-94c8-2e4b74c0d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "def objective(trial):\n",
    "    since = time.time()\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "    lr = trial.suggest_float('lr', 0.03, 1.05, step=0.03)    \n",
    "    batch_size = trial.suggest_int('batch_size', 200, 400, step=100)\n",
    "    out1 = trial.suggest_int('out1', 32, 128, step=4)\n",
    "    out2 = trial.suggest_int('out2', 4, 32, step=2)\n",
    "    \n",
    "    model = Net(size_columns, hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4, out1, out2).to(dev)\n",
    "       \n",
    "    train_dataload = data_torch.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataload = data_torch.DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    # Установить случайное начальное число\n",
    "    seed=42\n",
    "    torch.manual_seed(seed)\n",
    "           \n",
    "    # Обучение\n",
    "    n = 30\n",
    "    for epoch in range(n):\n",
    "        model.train()\n",
    "        for X, y in train_dataload:\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            features = len(next(iter(train_dataload))[0][0])\n",
    "            X = X.view(-1, features)\n",
    "            y = y.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X.float())\n",
    "            loss = criterion(output, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "    # Проверка\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_dataload:\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            features = len(next(iter(val_dataload))[0][0])\n",
    "            X = X.view(-1, features)\n",
    "            y = y.view(-1, 1)\n",
    "            output = model(X.float())\n",
    "            y_pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += y_pred.eq(y.view_as(y_pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(val_dataload.dataset)\n",
    "   \n",
    "    time_use = time.time() - since\n",
    "        \n",
    "    print('------------')\n",
    "    \n",
    "    print(\"Время на этап подбора：{:.0f}m{:.0f}s\".format(time_use//60,time_use%60))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7c3d2b-d840-4b2d-963e-ccf00b5ae3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Время на этап подбора：1m40s\n",
      "------------\n",
      "Время на этап подбора：1m36s\n",
      "Наилучшая accuracy:\n",
      "  accuracy:  0.6145251396648045\n",
      "  Параметры: \n",
      "    lr: 0.42000000000000004\n",
      "    batch_size: 300\n",
      "    out1: 32\n",
      "    out2: 8\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Создаем учебные объекты Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Оптимизация с использованием индикатора выполнения tqdm\n",
    "n_trials = 2 # Кол-во попыток\n",
    "\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "#study.optimize(objective, timeout=4) # 4 минуты перебора параметров\n",
    "\n",
    "print(\"Наилучшая accuracy:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  accuracy: \", best_trial.value)\n",
    "print(\"  Параметры: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58931252-4bc2-4fa2-adb9-756060e5e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (b1): Sequential(\n",
      "    (0): Linear(in_features=299, out_features=4485, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): BatchNorm1d(4485, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=4485, out_features=2093, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): BatchNorm1d(2093, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Linear(in_features=2093, out_features=1046, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.4, inplace=False)\n",
      "    (11): BatchNorm1d(1046, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Linear(in_features=1046, out_features=598, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Dropout(p=0.4, inplace=False)\n",
      "    (15): BatchNorm1d(598, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Linear(in_features=598, out_features=84, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): Linear(in_features=84, out_features=16, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (24): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(size_columns, hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4, study.best_trial.params['out1'], study.best_trial.params['out2'])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1d8c32-c7dc-4e2c-98b5-3c3dc2299057",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "epochs = 30\n",
    "\n",
    "batch_size = study.best_trial.params['batch_size']\n",
    "lr = study.best_trial.params['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4cab4b8-7c13-459d-9bc4-0859a63fca01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23ee0b790b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataload = data_torch.DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "val_dataload = data_torch.DataLoader(dataset=val_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "#test_dataload = data_torch.DataLoader(dataset=testing_dataset , batch_size=batch_size,shuffle=False, num_workers=num_workers)\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c0af5-4324-4e50-a139-ee1b1f33e705",
   "metadata": {
    "tags": []
   },
   "source": [
    "net = Net(size_columns, hidden_layer1, hidden_layer2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa13177-abc6-4369-9b3a-2d7661d764d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataload, val_dataload, epochs=10, lr=0.001):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(dev)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_acc, best_epoch = 0, 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    train_loss_all, val_loss_all = [], []\n",
    "    train_acc_all, val_acc_all = [], []\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_corrects, train_num = 0, 0, 0\n",
    "        val_loss, val_corrects, val_num = 0, 0, 0\n",
    "\n",
    "        print((\"\\n\" + \"%11s\" * 4) % (\"Epoch\", \"GPU_mem\", \"train_loss\", \"train_acc\"))\n",
    "        pbar = tqdm(train_dataload,bar_format='{l_bar}{bar:15}{r_bar}')\n",
    "\n",
    "        for X , y in pbar:\n",
    "            X , y = X.to(dev), y.to(dev)\n",
    "            model.train()\n",
    "            output = model(X)\n",
    "            loss = criterion(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred = torch.argmax(output,dim=1)\n",
    "\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            train_corrects += torch.sum(y_pred == y.data).item()\n",
    "            train_num += X.size(0)\n",
    "\n",
    "            mem = f\"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G\" \n",
    "            pbar.set_description(\n",
    "                (\"%11s\" *2 +\"%11.4g\" *2)\n",
    "                % (f\"{epoch+1}/{epochs}\",mem,train_loss/train_num,train_corrects/train_num)\n",
    "            )\n",
    "\n",
    "        pbar2 = tqdm(val_dataload,bar_format='{l_bar}{bar:15}{r_bar}')\n",
    "        \n",
    "        for X , y in pbar2:\n",
    "            X , y = X.to(dev) , y.to(dev)\n",
    "            model.eval()\n",
    "            output = model(X)\n",
    "            loss = criterion(output,y)\n",
    "\n",
    "            y_pred = torch.argmax(output,dim=1)\n",
    "\n",
    "            val_loss += loss.item() * X.size(0)\n",
    "            val_corrects += torch.sum(y_pred == y.data).item()\n",
    "            val_num += X.size(0)\n",
    "            pbar2.set_description(\n",
    "                ((\"%11s\" +\"%11.4g\")*2)\n",
    "                % (\"val_loss\",val_loss/val_num,\"val_acc\",val_corrects/val_num)\n",
    "            )\n",
    "        \n",
    "        train_loss_all.append(train_loss / train_num)\n",
    "        train_acc_all.append(train_corrects / train_num)\n",
    "        val_loss_all.append(val_loss / val_num)\n",
    "        val_acc_all.append(val_corrects / val_num)\n",
    "\n",
    "        if val_acc_all[-1] > best_acc:\n",
    "            best_epoch = epoch\n",
    "            best_acc = val_acc_all[-1]\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_use = time.time() - since\n",
    "    print(\"Общее время обучения：{:.0f}m{:.0f}s\".format(time_use//60,time_use%60))\n",
    "    print('best acc:',best_acc,'best epoch:',best_epoch)\n",
    "    \n",
    "    torch.save(best_model_wts,\"./best_model.csv\")\n",
    "\n",
    "    train_process = pd.DataFrame(data={\n",
    "        \"epoch\":range(epochs),\n",
    "        \"train_loss_all\":train_loss_all,\n",
    "        \"val_loss_all\":val_loss_all,\n",
    "        \"train_acc_all\":train_acc_all,\n",
    "        \"val_acc_all\":val_acc_all\n",
    "    })\n",
    "    return train_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b87baf-22b3-40ec-a6ef-4701403320ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Net(size_columns, hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4, study.best_trial.params['out1'], study.best_trial.params['out2'])\n",
    "train_process = train(model, train_dataload, val_dataload, epochs=epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839f13e-76cf-4007-a9cb-657c996e6c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_test_acc(model, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data:\n",
    "            X = X.squeeze()\n",
    "            y = y.squeeze()\n",
    "            prob_y = model(X.float())\n",
    "            y_pred = output.argmax(dim=1, keepdim=True)\n",
    "            total += len(y_pred)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "    return round((correct/total)*100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893810c-e418-45f6-9f05-c3163a5d7c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_test_acc(net, train_dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe242a-3678-48f0-9362-b1577ec0084a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_test_acc(net, val_dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00388e1d-81c0-40fc-9598-c03ffd9331af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
