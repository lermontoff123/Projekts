{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f776321-84bb-4016-a9a4-ea0a58234d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils.data as data_torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ydata_profiling import ProfileReport # подробный разбор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a647c34-9f04-4dc2-b7c4-1598bdbdbc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_patch = '.\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e374b48-0d0b-4211-a90e-489f6e77325d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratify_data(filename):\n",
    "    \n",
    "    dataframe = pd.read_csv(filename).copy()\n",
    "    dataframe = dataframe.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    mask = dataframe['Age'].isna()\n",
    "    dataframe.loc[mask, 'Age'] = np.random.randint(10, 50, mask.sum())\n",
    "    dataframe['Embarked'] = dataframe['Embarked'].fillna(dataframe['Embarked'].mode()[0])\n",
    "    dataframe['Sex'] = dataframe['Sex'].map({'male':1,'female':0})\n",
    "    dataframe['Embarked'] = dataframe['Embarked'].map({'S':0,'C':1, 'Q':2})\n",
    "    \n",
    "    survived = dataframe['Survived']\n",
    "    #survived = pd.concat([survived, survived.copy()], ignore_index=True)\n",
    "    features = dataframe.drop(['Survived'], axis=1)\n",
    "    \n",
    "    features = pd.get_dummies(features, columns=['Pclass', 'SibSp', 'Parch', 'Embarked'], dtype=int)\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    features = poly.fit_transform(features)\n",
    "        \n",
    "    features = RobustScaler().fit_transform(features)\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    \n",
    "    data_scaled = pd.DataFrame(data=features)\n",
    "    #data_scaled = pd.concat([data_scaled, data_scaled.copy()], ignore_index=True)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(data_scaled, survived, test_size=0.2, random_state=2, stratify=survived)\n",
    "    \n",
    "    X_train['Survived'] = y_train\n",
    "    X_val['Survived'] = y_val\n",
    "    \n",
    "    X_train.to_csv('train_data.csv', sep=',', index=False)\n",
    "    X_val.to_csv('val_data.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3d53fd-ff2c-4f79-8b6a-a89f73bd6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify_data(data_patch + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d792d296-1050-4133-8c46-fce206bb4510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TitanicDataset(data_torch.Dataset):\n",
    "    \n",
    "    def __init__(self, filename, Train=True):\n",
    "        self.dataframe = pd.read_csv(filename).copy()\n",
    "        self.Train = Train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if(self.Train):\n",
    "            survived = self.dataframe['Survived']\n",
    "            survived = np.array(survived)[idx]\n",
    "            \n",
    "        features = self.dataframe.drop(['Survived'], axis=1)\n",
    "        features = np.array(features)[idx]\n",
    "                    \n",
    "        if(self.Train):\n",
    "            return features, survived\n",
    "        else:\n",
    "            return features\n",
    "          \n",
    "    def infoo(self):\n",
    "        return self.dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65d665d3-9c21-478c-8ae1-18b335c0310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TitanicDataset(data_patch + 'train_data.csv')\n",
    "val_dataset = TitanicDataset(data_patch + 'val_data.csv')\n",
    "testing_dataset = TitanicDataset(data_patch + 'test.csv', Train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8706a68a-192e-40ea-890d-4f2c373029c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00000000e+00, 3.34003518e-01, 2.70496001e-02, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.13881826e-01, 2.02872001e-02,\n",
       "        0.00000000e+00, 3.85714286e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.85714286e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.37500000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.80281690e-01, 0.00000000e+00,\n",
       "        7.31680868e-04, 0.00000000e+00, 1.88548299e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.26931559e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.70496001e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.70496001e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3d3db0-e7f3-4365-b45e-6c9775f704da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 712 entries, 0 to 711\n",
      "Columns: 300 entries, 0 to Survived\n",
      "dtypes: float64(299), int64(1)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.infoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "563bb03d-f51c-436c-8655-784a80a21602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.0000, 0.1455, 0.0303,  ..., 0.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 0.1078, 0.0401,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0450, 0.0610,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0000, 0.7110, 0.0241,  ..., 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.5225, 0.4441,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.1958, 0.0205,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64), tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "train_dataload = data_torch.DataLoader(train_dataset, shuffle=True, batch_size=100)\n",
    "t = iter(train_dataload)\n",
    "print(next(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e57b904-14d7-4bc8-bfeb-e54272585dca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.1958]]],\n",
      "\n",
      "\n",
      "         [[[0.0497]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.3089]]],\n",
      "\n",
      "\n",
      "         [[[0.1082]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.7487]]],\n",
      "\n",
      "\n",
      "         [[[0.0518]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.2963]]],\n",
      "\n",
      "\n",
      "         [[[0.0314]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.3968]]],\n",
      "\n",
      "\n",
      "         [[[0.1489]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.2712]]],\n",
      "\n",
      "\n",
      "         [[[0.0141]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[1.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.0000]]]]], dtype=torch.float64), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "val_dataload = data_torch.DataLoader(val_dataset, shuffle=True, batch_size=100)\n",
    "t = iter(val_dataload)\n",
    "print(next(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19de4e7e-7a97-4dec-abd9-18093efd4eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size_columns = len(next(iter(train_dataload))[0][0])\n",
    "hidden_layer1_coeff = 15\n",
    "hidden_layer2_coeff = 7\n",
    "hidden_layer3_coeff = 3.5\n",
    "hidden_layer4_coeff = 2\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "hidden_layer1 = round(hidden_layer1_coeff * size_columns)\n",
    "hidden_layer2 = round(hidden_layer2_coeff * size_columns)\n",
    "hidden_layer3 = round(hidden_layer3_coeff * size_columns)\n",
    "hidden_layer4 = round(hidden_layer4_coeff * size_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58931252-4bc2-4fa2-adb9-756060e5e778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = models.Swin_S_Weights.DEFAULT\n",
    "transfer_model = models.swin_s(weights=weights)\n",
    "transfer_model.features\n",
    "transfer_model.head = nn.Linear(transfer_model.head.in_features, out_features=1)\n",
    "transfer_model.features[0][0] = nn.Conv2d(size_columns, 96, kernel_size=(4, 4), stride=(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60162a-3cc6-4156-a0ee-907f8cdc796b",
   "metadata": {
    "tags": []
   },
   "source": [
    "transfer_model.features[0] = nn.Sequential(\n",
    "    nn.Linear(size_columns, 96),\n",
    "    nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3726b8af-5ec0-4ec1-8b50-883f9e277bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Permute()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model.features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94441125-c7dc-4072-809d-be62dfb11f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Замораживаем веса предобученных слоев\n",
    "for param in transfer_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Обучаем только новые полносвязные слои\n",
    "transfer_model.head.weight.requires_grad = True\n",
    "transfer_model.head.bias.requires_grad = True\n",
    "transfer_model.features[0][0].weight.requires_grad = True\n",
    "transfer_model.features[0][0].bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35969110-ee8b-4be0-a99e-4e5b81a34eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(transfer_model.parameters(), lr = lr, momentum = momentum)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr = lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "722a230d-d158-4ce8-abdb-2c7ca658aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, optimizer, criterion, epochs=10):\n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(dev)\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        batch = 0\n",
    "        for  X, y in training_data:\n",
    "            X , y = X.to(dev) , y.to(dev)\n",
    "            \n",
    "            features = len(next(iter(training_data))[0][0])\n",
    "            X = X.view(-1, features)\n",
    "            y = y.view(-1, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            prob_y = model(X.float())\n",
    "            \n",
    "            y_pred = (prob_y > torch.Tensor([0.5])).long().squeeze()\n",
    "            \n",
    "            accuracy.append((y_pred==y).sum().item()/len(y_pred))\n",
    "            \n",
    "            loss = criterion(prob_y, y.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            batch += 1\n",
    "        # средние потери на каждой эпохе    \n",
    "        losses.append(running_loss/batch)\n",
    "        if epoch%5==0:\n",
    "            print('epochs {} done'.format(epoch+5))\n",
    "        \n",
    "    print(\"Fin\")\n",
    "    return (losses, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86b56074-2bc1-47cc-8a79-12b0fb2a1c7b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [100, 299]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses, accuracy \u001b[38;5;241m=\u001b[39m train(transfer_model, train_dataload, optimizer, criterion, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "Cell \u001b[1;32mIn[46], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, training_data, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m prob_y \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (prob_y \u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0.5\u001b[39m]))\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     22\u001b[0m accuracy\u001b[38;5;241m.\u001b[39mappend((y_pred\u001b[38;5;241m==\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_pred))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\swin_transformer.py:608\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 608\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m    609\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    610\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute(x)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [100, 299]"
     ]
    }
   ],
   "source": [
    "losses, accuracy = train(transfer_model, train_dataload, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5b2ec-cd34-469a-bb28-230c222866bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(losses, accuracy):\n",
    "    figure = plt.figure(figsize=(9,3))\n",
    "    plt.subplot(121, title=\"losses\")\n",
    "    plt.plot(range(1, len(losses)+1), losses)\n",
    "    plt.xlabel(\"epoch №\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.subplot(122, title=\"accuracy\")\n",
    "    plt.plot(range(1, len(accuracy)+1), accuracy)\n",
    "    plt.xlabel(\"batch №\")\n",
    "    plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f03dbf-ccf2-4087-8c0c-dd64cfad2e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(losses, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a4e7c-894f-46b5-82d4-988028f13109",
   "metadata": {
    "tags": []
   },
   "source": [
    "MODEL_PATH = '.\\\\titanic_transfer_model.pth'\n",
    "torch.save(transfer_model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40387c31-b99e-4e4a-8cbf-49c2ba3fb1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_test_acc(model, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data:\n",
    "            X = X.squeeze()\n",
    "            y = y.squeeze()\n",
    "            prob_y = model(X.float())\n",
    "            y_pred = (prob_y > torch.Tensor([0.5])).long().squeeze()\n",
    "            total += len(y_pred)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "    return round((correct/total)*100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d8141-15a7-483e-86c8-34faea0b5575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_test_acc(transfer_model, train_dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a313491-1694-4f9d-a930-8378488b036f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calc_test_acc(transfer_model, val_dataload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e0aa50-944b-4a31-b07f-8bd733a10b33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Callback',\n",
       " 'LIGHTNING_LOGO',\n",
       " 'LightningDataModule',\n",
       " 'LightningModule',\n",
       " 'Trainer',\n",
       " '__about__',\n",
       " '__all__',\n",
       " '__annotations__',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__docs__',\n",
       " '__docs_url__',\n",
       " '__file__',\n",
       " '__homepage__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_graveyard',\n",
       " '_logger',\n",
       " '_root_logger',\n",
       " 'accelerators',\n",
       " 'callbacks',\n",
       " 'cli_lightning_logo',\n",
       " 'core',\n",
       " 'disable_possible_user_warnings',\n",
       " 'loggers',\n",
       " 'logging',\n",
       " 'loops',\n",
       " 'module_available',\n",
       " 'os',\n",
       " 'overrides',\n",
       " 'plugins',\n",
       " 'profilers',\n",
       " 'pytorch_lightning',\n",
       " 'seed_everything',\n",
       " 'strategies',\n",
       " 'trainer',\n",
       " 'tuner',\n",
       " 'utilities']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "dir(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05b4bf8-a4b4-4f4d-9b17-c12a48d8fe39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pl.models.base import LightningModule\n",
    "from pl.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pl.trainer import Trainer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv('titanic.csv')\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "X = data[features]\n",
    "y = data['Survived']\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Использование предобученной модели из PyTorch Lightning\n",
    "class TitanicModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCELoss()(y_hat, y.unsqueeze(1))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCELoss()(y_hat, y.unsqueeze(1))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_pred = (y_hat > 0.5).float()\n",
    "        accuracy = (y_pred == y).float().mean()\n",
    "        return accuracy\n",
    "\n",
    "model = TitanicModel()\n",
    "trainer = Trainer(accelerator='gpu', devices=1, max_epochs=10, callbacks=[EarlyStopping(monitor='val_loss')])\n",
    "trainer.fit(model, (X_train, y_train), (X_test, y_test))\n",
    "\n",
    "# В этом примере, мы используем предварительно обученную модель resnet18 из PyTorch, которая была обучена на наборе данных ImageNet. Мы адаптируем последний полносвязный слой модели для решения задачи классификации выживших пассажиров на Титанике.\n",
    "\n",
    "# Вы можете также попробовать использовать другие предобученные модели, доступные в PyTorch, такие как vgg16, densenet121 или mobilenet_v2, в зависимости от характеристик ваших данных.\n",
    "\n",
    "# Использование предобученных моделей может значительно ускорить и упростить процесс обучения, особенно если у вас нет больших наборов данных или вычислительных ресурсов для обучения модели с нуля."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
